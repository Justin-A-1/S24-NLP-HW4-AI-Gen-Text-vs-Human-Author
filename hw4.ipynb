{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE TO GRADER: It is my understanding tht a pdf of my Jupyter Notebook is sufficient for submission. Please let me know if you would like, and I am happy to send you my code. Have a great day! - Justin A\n",
    "\n",
    "NOTE TO GRADER: Analysis of Part 2, 3, and 4 is all in part 5 at the end. Please also see the additional PDF containing result tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1: \n",
    "- Go to Kaggle.com. \n",
    "- Find a text classification data set that interests you. Divide into train/test.\n",
    "- Create a graph showing the distribution of the target classes. \n",
    "- Describe the data set and what the model should be able to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCRIPTION OF DATA SET + WHAT MODEL SHOULD BE ABLE TO PREDICT\n",
    "\n",
    "- The original data set consists of 487K+ text entries that are either written by humans or AI generated. \n",
    "- Distribution: 306K by human author, 181K AI gen. \n",
    "- For this assignment, the first 120K entries were used. \n",
    "- Distribution: 66K human author, 54K AI Gen. \n",
    "- The model should be able to predict the category: human author or AI generated.\n",
    "- Source: https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT DATA FROM .CSV FILE, AND PUT IT IN LIST \"DATA\"\n",
    "import csv\n",
    "import os\n",
    "from itertools import islice\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "dataset_filename = \"AI_Human.csv\"\n",
    "dataset_file_path = os.path.join(current_directory, dataset_filename)\n",
    "\n",
    "data = []\n",
    "with open(dataset_file_path, 'r', encoding = 'utf-8') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    for row in islice(csv_reader,120000):\n",
    "        # Append csv row to data list\n",
    "        data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Several values of x_train:\n",
      "no one is evan going to read these things so there is no point in doing this test it is wasting ever\n",
      "\n",
      "\n",
      "Car-free cities, as the term itself suggests, imply urban environments where automobiles play a li\n",
      "Studies have shown that enforcing curfews for tanagers can half caducei they likelihood of that gett\n",
      "My opinion is that we be able to vote saying the most popular votes win because why would we give ar\n",
      "The Advantages and Disadvantages of Using Technology in the Classroom\n",
      "\n",
      "As technology Continues to ad\n",
      "Dear State Senator,\n",
      "\n",
      "As a 9th grade student, I am writing to express my opinion on the matter of the\n",
      "Honestly this would raise test scores because you are encouraging student to work harder, so they ca\n",
      "Technology has revolutionized the way we live Our lives, and its impact Kn individuals' lives has be\n",
      " Have you ever wondered what would happen if you knew more about something you liked, something you \n",
      " As we journey through life, OT Os essential to cultivate the right mindset and skills that along wi\n",
      "\n",
      "Several values of y_train:\n",
      "[0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# SPLIT DATA INTO TRAIN AND TEST\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract 'headline' and 'is_sarcastic' from each dictionary in the data list\n",
    "text = [entry['text'] for entry in data]\n",
    "ai_generated = [int(float(entry['generated'])) for entry in data]\n",
    "\n",
    "# Split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(text, ai_generated, test_size=0.10, random_state=92)\n",
    "\n",
    "print(\"Several values of x_train:\")\n",
    "for text in x_train[:10]:\n",
    "    print(text[:100])\n",
    "\n",
    "print(\"\\nSeveral values of y_train:\")\n",
    "print(y_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai_gen_count:54344\n",
      "not_ai_gen_count:65656\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOT THE CLASSIFICATION DISTRIBUTION IN A HISTOGRAM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# count occurrences of each class (0: not ai_generated, 1: ai_generated)\n",
    "ai_gen_count = sum(y_train) + sum(y_test)\n",
    "not_ai_gen_count = len(y_train) + len(y_test) - ai_gen_count\n",
    "\n",
    "print(f\"ai_gen_count:{ai_gen_count}\")\n",
    "print(f\"not_ai_gen_count:{not_ai_gen_count}\")\n",
    "\n",
    "# graph showing distribution of target classes\n",
    "plt.bar(['Human Author', 'AI Gen'],[not_ai_gen_count, ai_gen_count])\n",
    "plt.title('Class Distribution:')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2: CREATE A SEQUENTIAL MODEL AND EVALUATE ON THE TEST DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTEMPT 1: \"BASIC SEQUENTIAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 1\n",
      "to: 2\n",
      "and: 3\n",
      "a: 4\n",
      "of: 5\n",
      "in: 6\n",
      "that: 7\n",
      "is: 8\n",
      "it: 9\n",
      "for: 10\n",
      "x_train_padded:[[111  49   8 148   2  70 122  61  37   8 111   6 188  18   9   8  57  14\n",
      "    3  29   4   5  16   4  29  14  19  84 104  16   1   8  61  85 110  40\n",
      "   25  17   1 105 191  61  47   9   2  81  15  63   2 109  37 125   2 134\n",
      "   24  13  50  40  63  15 102  72   1   5 109  17  17   2  85  22  37   3\n",
      "   17 111 111  77  29  29  32 175  18 112   8   3  29  14   1 188   9 111\n",
      "    1  19   1  19   1  41 109   1 138   8]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, convert to sequences, and pad text to be ready for neural network\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "\n",
    "# initialize and fit the tokenizer on the training data\n",
    "num_words1 = 200\n",
    "tokenizer = Tokenizer(num_words = num_words1)#10000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "for word, index in islice(tokenizer.word_index.items(), 10):\n",
    "    print(f'{word}: {index}')\n",
    "\n",
    "# convert texts to sequences\n",
    "x_train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# pad the text sequences\n",
    "max_length = 100\n",
    "x_train_padded = pad_sequences(x_train_sequences, maxlen = max_length, padding = 'post', truncating = 'post')\n",
    "x_test_padded = pad_sequences(x_test_sequences, maxlen = max_length, padding = 'post', truncating = 'post')\n",
    "\n",
    "# convert to numpy arrays\n",
    "x_train_padded = np.array(x_train_padded)\n",
    "x_test_padded = np.array(x_test_padded)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f'x_train_padded:{x_train_padded[:1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.9395 - loss: 0.1603 - precision_1: 0.9521 - recall_1: 0.9104 - val_accuracy: 0.9703 - val_loss: 0.0827 - val_precision_1: 0.9699 - val_recall_1: 0.9649\n",
      "Epoch 2/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.9710 - loss: 0.0784 - precision_1: 0.9735 - recall_1: 0.9624 - val_accuracy: 0.9737 - val_loss: 0.0731 - val_precision_1: 0.9866 - val_recall_1: 0.9552\n",
      "Epoch 3/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9760 - loss: 0.0661 - precision_1: 0.9782 - recall_1: 0.9687 - val_accuracy: 0.9797 - val_loss: 0.0585 - val_precision_1: 0.9791 - val_recall_1: 0.9762\n",
      "Epoch 4/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.9813 - loss: 0.0540 - precision_1: 0.9824 - recall_1: 0.9761 - val_accuracy: 0.9787 - val_loss: 0.0567 - val_precision_1: 0.9844 - val_recall_1: 0.9688\n",
      "Epoch 5/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.9828 - loss: 0.0475 - precision_1: 0.9842 - recall_1: 0.9778 - val_accuracy: 0.9797 - val_loss: 0.0563 - val_precision_1: 0.9871 - val_recall_1: 0.9680\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9787 - loss: 0.0609 - precision_1: 0.9872 - recall_1: 0.9654\n",
      "{'loss': 0.05628989264369011, 'compile_metrics': 0.9796666502952576}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = num_words1, output_dim = 128))# output_dim in the table refers tthis output_dim\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics= ['accuracy', Precision(), Recall()])\n",
    "\n",
    "#set up early stopping so that the model stops if test metrics stop improving (to avoid overfitting)\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "model.fit(x_train_padded, y_train, epochs=5, validation_data = (x_test_padded, y_test))#, callbacks=[early_stopping])\n",
    "\n",
    "results = model.evaluate(x_test_padded, y_test)\n",
    "print(dict(zip(model.metrics_names, results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 3: TRY A DIFFERENT ARCHITECTURE LIKE RNN, CNN, LSTM, ETC AND EVALUATE ON THE TEST DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTEMPT 2: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 18ms/step - accuracy: 0.8090 - loss: 0.4142 - val_accuracy: 0.8937 - val_loss: 0.3176\n",
      "Epoch 2/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 17ms/step - accuracy: 0.7993 - loss: 0.4436 - val_accuracy: 0.9150 - val_loss: 0.2622\n",
      "Epoch 3/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 18ms/step - accuracy: 0.8209 - loss: 0.4044 - val_accuracy: 0.9030 - val_loss: 0.2727\n",
      "Epoch 4/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 18ms/step - accuracy: 0.8099 - loss: 0.4268 - val_accuracy: 0.7418 - val_loss: 0.5459\n",
      "Epoch 5/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 20ms/step - accuracy: 0.8307 - loss: 0.4075 - val_accuracy: 0.8478 - val_loss: 0.3347\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8446 - loss: 0.3387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3347378373146057, 0.8477500081062317]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = num_words1, output_dim = 128))#, input_length = max_length))\n",
    "model.add(SimpleRNN(128))\n",
    "#model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "model.fit(x_train_padded, y_train, epochs=5, validation_data = (x_test_padded, y_test))#, callbacks=[early_stopping])\n",
    "\n",
    "model.evaluate(x_test_padded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTEMPT 3: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 18ms/step - accuracy: 0.9415 - loss: 0.1407 - precision_2: 0.9509 - recall_2: 0.9135 - val_accuracy: 0.9861 - val_loss: 0.0405 - val_precision_2: 0.9889 - val_recall_2: 0.9804\n",
      "Epoch 2/2\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 19ms/step - accuracy: 0.9886 - loss: 0.0337 - precision_2: 0.9895 - recall_2: 0.9853 - val_accuracy: 0.9897 - val_loss: 0.0301 - val_precision_2: 0.9914 - val_recall_2: 0.9859\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9866 - loss: 0.0383 - precision_2: 0.9890 - recall_2: 0.9812\n",
      "{'loss': 0.030137520283460617, 'compile_metrics': 0.9896666407585144}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words1, output_dim=128))\n",
    "model.add(Conv1D(128, 5, activation='relu'))  # Convolutional layer\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics= ['accuracy', Precision(), Recall()])\n",
    "\n",
    "model.fit(x_train_padded, y_train, epochs=2, validation_data = (x_test_padded, y_test))#, callbacks=[early_stopping])\n",
    "\n",
    "results = model.evaluate(x_test_padded, y_test)\n",
    "print(dict(zip(model.metrics_names, results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTEMPT 4: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 54ms/step - accuracy: 0.9303 - loss: 0.1784 - val_accuracy: 0.9703 - val_loss: 0.0878\n",
      "Epoch 2/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 55ms/step - accuracy: 0.9700 - loss: 0.0874 - val_accuracy: 0.9688 - val_loss: 0.0910\n",
      "Epoch 3/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 54ms/step - accuracy: 0.9762 - loss: 0.0678 - val_accuracy: 0.9749 - val_loss: 0.0688\n",
      "Epoch 4/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 55ms/step - accuracy: 0.9819 - loss: 0.0533 - val_accuracy: 0.9817 - val_loss: 0.0524\n",
      "Epoch 5/5\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 55ms/step - accuracy: 0.9856 - loss: 0.0428 - val_accuracy: 0.9848 - val_loss: 0.0491\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.9833 - loss: 0.0539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0491107739508152, 0.9848333597183228]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words1, output_dim=128))\n",
    "model.add(LSTM(128))  # LSTM layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "model.fit(x_train_padded, y_train, epochs=5, validation_data = (x_test_padded, y_test))#, callbacks=[early_stopping])\n",
    "\n",
    "model.evaluate(x_test_padded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 4: TRY DIFFERENT EMBEDDING APPROACHES FOR YOUR BEST PERFORMING MODEL IN STEP 3, AND EVALUATE ON THE TEST DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST PERFORMING MODEL: CNN WITH 2 EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTEMPT 5: DIFFERENT EMBEDDING APPROACHES WITH CNN WITH 2 EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.8956 - loss: 0.2500 - precision_10: 0.9001 - recall_10: 0.8550 - val_accuracy: 0.9387 - val_loss: 0.1601 - val_precision_10: 0.9351 - val_recall_10: 0.9300\n",
      "Epoch 2/2\n",
      "\u001b[1m3375/3375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - accuracy: 0.9382 - loss: 0.1655 - precision_10: 0.9365 - recall_10: 0.9261 - val_accuracy: 0.9269 - val_loss: 0.1850 - val_precision_10: 0.8796 - val_recall_10: 0.9730\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9284 - loss: 0.1876 - precision_10: 0.8808 - recall_10: 0.9733\n",
      "{'loss': 0.1850082129240036, 'compile_metrics': 0.9269166588783264}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words1, output_dim=2)) #output_dim in the table refers to this line\n",
    "model.add(Conv1D(128, 5, activation='relu'))  # Convolutional layer\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics= ['accuracy', Precision(), Recall()])\n",
    "\n",
    "model.fit(x_train_padded, y_train, epochs=2, validation_data = (x_test_padded, y_test))#, callbacks=[early_stopping])\n",
    "\n",
    "results = model.evaluate(x_test_padded, y_test)\n",
    "print(dict(zip(model.metrics_names, results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 5: ANALYSIS OF PREVIOUS PARTS\n",
    "- See additional PDF for evaluation tables.\n",
    "\n",
    "ANALYSIS OF PART 2:\n",
    "- While not explitly required, I decided to test 21 different versions of my original sequential model. As shown in the additional PDF's evaluation tables, I varied epochs, Num. of Words in Tokenizer, and Text Sequence Max Length. (Here, Num. of Words in tokenizer is how many of the most frequent words across training data were used in the model. Text Sequence Max Length is the length of the test sequence used to classify each instance as AI or human authored.)\n",
    "- Using accuracy by itself, I compared different options and selected one\n",
    "- After that, I calculated precision and recall of the selected option to confirm they were good\n",
    "- This is a remarkable data set - even with only 1 epoch, 5 words in the tokenizer, and a text sequence length of 5, it still achieved ***59% accuracy***! That's astonishing.\n",
    "- Full results can be seen in the additional PDF's table.\n",
    "- Increasing epochs from 1 to 5 to 10 had almost no impact\n",
    "- Num. of Words in Tokenizer and Text Sequence Max Length were increased together, and each increase had a noticeable impact until diminishing returns were hit at 800 words in tokenizer and text sequence max length of 100. This combination achieved 98.18% accuracy, so further returns were increasingly diminished simply because there wasn't much room left to improve. \n",
    "- Even 10000 words in tokenizer only did a little better.\n",
    "- With 800 words in tokenizer and text sequence max length of 100 chosen, precision and recall were found to be a reasonable 98.7% and 96.5%\n",
    "- Overall, these 96-98% results for accuracy/precision/recall indicate that the data set certainly does have sufficient information to classify nearly all data entries properly.\n",
    "- Again, see the additional PDF's table for full results.\n",
    "- This 800+100 pair of parameters, with 5 epochs, was chosen for comparison in part 3.\n",
    "\n",
    "ANALYSIS OF PART 3:\n",
    "- The following models were tried at 800 words in tokenizer, 100 text sequence max length, and 5 and sometimes 2 epochs: RNN+Dense Layer, RNN, CNN, LSTM\n",
    "- Full results can be seen in the additional PDF's table.\n",
    "- The basic sequential model, from Part 2, was the benchmark with 98.183% accuracy\n",
    "- RNN+Dense layer performed most poorly, with 70.7% accuracy! This is likely due to overfitting.\n",
    "- RNN was next most poor performing, with 84.8% accuracy. Again, overfitting may play a role here.\n",
    "- Since the two worst performing models were the RNNs, it is likely that their ability to maintain \"memory\" of previous inputs is actually harmful in this exercise. What does that mean? It means that remembering and focusing on key words from way way back early in the text actually makes it harder for the model to distinguish between human author and AI author. For whatever reason, these results suggest that a long memory makes human and AI authored works seem more similar - where as the basic sequential model, which is more focused on word counts overall, does not run into this trouble and hence earns more accuracy.\n",
    "- But, oddly, LSTM performed slightly better than basic sequential (about 0.4% higher accuracy), which strongly implies that it's long term memory was not a hidrance, and possibly even a minor asset. I have no idea why this is. It may be due to: an uncaught bug in the code, some unidentified weakness in RNNs unrelated to my early thoughts on them, or something else entirely. Perhaps the Tensorflow Keras LSTM model has some ability to turn off it's memory that the RNNs lack. That would allow it to bypass the problems I outlined in the previous bullet. This whole issue is a head scratcher.\n",
    "- Finally, the two best performing models were CNN with 5 and 2 epochs. This may indicate that their ability to recognize spatial patterns in the data may give them a slight edge.\n",
    "- Bizarrely, the 2 epoch CNN outperformed the 5 epoch CNN. It seems likely that this is due either to slight over fitting, or else is due to random noise. (their accuracies are very close to each other - within 0.017%!)\n",
    "- The small number of epochs is likely because the data set - 120K entries - is huge, meaning that each epoch covers a lot of ground.\n",
    "- Frankly, sharing 800 words in tokenizer and 100 text sequence max length, all of basic sequential, LSTM, and CNN performed between 98% and 99%, which seems to indicate that the model choice was relatively unimportant. Regardless of the model, the data left siginificant enough \"clues\" that multiple models were able to achieve simliarly high accuracy.\n",
    "- Nonetheless, the CNN with 2 epochs model was chosen \n",
    "\n",
    "ANALYSIS OF PART 4:\n",
    "- Finally, different embeding approaches were done. First, the number of words in teh tokenizer was varied from 800 down to just 5. With a number selected, the next cange was to vary the output dimensions of the embedding layer from 128 down to just 2.\n",
    "- See full results in the additional PDF's table.\n",
    "- Number of words in tokenizer: Accuracy did not decrease significantly from 800 to 600 to 400 and even down to 200. Only at 100 and below did accuracy begin to decrease more significantly. So, 200 words in tokenizer was selected for efficiency.\n",
    "- Output dimensions of the embedding layer was 128 for all previous models mentioned in Parts 2, 3, and 4.\n",
    "- Here, it was singularly bumped up to 256, and also decreased to 64, 32, 16, 8, 4, and 2.\n",
    "- Accuracy loss was minimal down to output_dim = 16, so this was chosen for efficiency.\n",
    "- The 200 words in tokenizer, 16 output dimensions for embedding layer, 2 epoch CNN model was the final model chosen. Accuracy: 97.175%, Precision: 97.060%, Recall: 97.080%\n",
    "- What can we learn from these parameters?\n",
    "- First, the 200 words in tokenizer maintaining high accuracy suggests that when words across all samples are sorted by frequency, only the top 200 most frequent words are needed to classify the data with a high accuracy, precision, etc. In other words, word 201 and on are just not that important.\n",
    "- It may be true that very few of the top 200 words are important. Testing was not done on top 120, 140, 160, 180, etc - the closest lower grouping was 100. But, 100 did suffer lower accuracy, indicating that on top of several words in the top 100 being important, at least *some* word in the 101-200 range is also important\n",
    "- The 16 output dimensions for embedding layer suggests that whatever important information this list of 200 words contains - it is quite compressible. Perhaps only a handful of the top 200 words are needed, or perhaps more are needed but the equations somehow compress them.\n",
    "- It was determined in Part 2, but it's worth noting that the 100 word max text sequence indicates that the classification's hand is sufficiently \"revealed\" after just 100 key words from the top 200 list. Additional words provide minimal improvement.\n",
    "\n",
    "OVERALL TAKEAWAY\n",
    "- Overall, this AI text generator is bad at it's job. A simple separate AI programmed by a grad student for a homework assignment can sniff out this AI text generator with 98% accuracy through a not-that-complicated process. That means this AI is leaving clear clues that human authors are not. The poor quality of the original AI text generator - it's inability to hide itself amongst real human writing - is probably the plainest takeaway from this investigation. While it may fool humans, when another AI comes to snoop for clues, they stick out as clearly as a ketchup stain on a white shirt.\n",
    "- Surely analysis of my neural networks would reveal the clues they find, which themselves would point the way towards making an AI that can mimic human authors more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
